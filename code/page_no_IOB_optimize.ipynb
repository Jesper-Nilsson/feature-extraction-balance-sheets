{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257b3c6-c46e-4f9f-9f32-1ea71d700bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, ClassLabel\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer, \\\n",
    "    AutoModelForTokenClassification, AutoConfig\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")\n",
    "\n",
    "# Define the ClassLabel for consistent label handling\n",
    "classmap = ClassLabel(\n",
    "    num_classes=6,\n",
    "    names=['O', 'ASSET', 'DEBT', 'EQUITY', 'DEPOSIT', 'MISC']\n",
    "    # Ensure 'O' is included and at the correct index if used often\n",
    ")\n",
    "# Create label to ID and ID to label mappings\n",
    "label2id = {label: classmap.str2int(label) for label in classmap.names}\n",
    "id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "\n",
    "def label_data(jsondata, tokenizer, label2id):\n",
    "    token_list = []\n",
    "    label_list = []\n",
    "    attention_mask_list = []\n",
    "    for item in jsondata:\n",
    "        file_path = item['text']\n",
    "        labels = item.get('label', [])  # Handle missing labels\n",
    "        with open(f'test_set/{file_path}', 'r') as file:\n",
    "            text = file.read()\n",
    "             \n",
    "        tokenized_input = tokenizer(text, return_offsets_mapping=True, return_attention_mask=True)\n",
    "        tokens = tokenized_input['input_ids']\n",
    "        offset_mapping = tokenized_input['offset_mapping']\n",
    "        attention_mask_list.append(tokenized_input['attention_mask'])\n",
    "\n",
    "        token_labels = [label2id['O']] * len(tokens)  # Default label is 'O'\n",
    "\n",
    "        if labels:\n",
    "            for label in labels:\n",
    "                start_label = label['start']\n",
    "                end_label = label['end']\n",
    "                entity_label = label['labels'][0]\n",
    "                for idx, (start, end) in enumerate(offset_mapping):\n",
    "                    if start < end_label and end > start_label:\n",
    "                        token_labels[idx] = label2id[entity_label]\n",
    "\n",
    "        token_list.append(tokens)\n",
    "        label_list.append(token_labels)\n",
    "\n",
    "    return token_list, label_list, attention_mask_list\n",
    "\n",
    "\n",
    "def process_json_file(filename, tokenizer, label2id):\n",
    "    with open(filename, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    token_list, label_list, attention_mask_list = label_data(json_data, tokenizer, label2id)\n",
    "    return pd.DataFrame({\n",
    "        'input_ids': token_list,\n",
    "        'labels': label_list,\n",
    "        'attention_mask': attention_mask_list\n",
    "    })\n",
    "\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = 'context_balance.json'\n",
    "processed_data_2 = process_json_file(json_file_path, tokenizer, label2id)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize a Counter to count occurrences of each label\n",
    "label_counter2 = Counter()\n",
    "\n",
    "# Iterate over the label lists in the processed data\n",
    "for labels in processed_data_2['labels']:\n",
    "    # Update the counter with the labels in the current list\n",
    "    label_counter2.update(labels)\n",
    "\n",
    "# Print the counts of each label\n",
    "for label, count in label_counter2.items():\n",
    "    print(f\"Label '{id2label[label]}' appears {count} times.\")\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(processed_data)\n",
    "dataset.set_format('torch')\n",
    "datasetDict = dataset.train_test_split(0.2)\n",
    "\n",
    "print(\"dataset prepared\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda0eae-6da8-47fb-aaac-7a46eff216c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Extract predictions and labels from the outputs\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "    true_labels = p.label_ids\n",
    "    \n",
    "    # Flatten the predictions and true_labels to handle them with sklearn metrics\n",
    "    # Also, remove ignored indices (-100 used for padding or special tokens)\n",
    "    true_labels_flat = [label for sublist in true_labels for label in sublist if label != -100]\n",
    "    predictions_flat = [pred for sublist, true_list in zip(predictions, p.label_ids)\n",
    "                        for pred, true in zip(sublist, true_list) if true != -100]\n",
    "\n",
    "    # Calculate metrics using sklearn\n",
    "    precision = precision_score(true_labels_flat, predictions_flat, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels_flat, predictions_flat, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels_flat, predictions_flat, average='macro', zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels_flat, predictions_flat)  # Optional\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy  # Optional\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20295e2a-ea14-476f-8050-221f1a5e3c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415a4fc-b867-42f3-ade7-201791079ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoConfig\n",
    "from datasets import load_metric\n",
    "import optuna\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    # Sample hyperparameters using trial object\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 5e-3, log=True)\n",
    "    #, 64,128\n",
    "    batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16, 32])\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 1, 10)\n",
    "    warmup_steps = trial.suggest_int('warmup_steps', 0, 500)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"polynomial\"])\n",
    "\n",
    "    def model_init():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        model_name = \"KB/bert-base-swedish-cased-ner\"  # Example model, replace with your specific model\n",
    "        \n",
    "        \n",
    "        # Update the config with the number of labels you have\n",
    "        num_labels = len(label2id)  # This should be set up from your ClassLabel or similar configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.num_labels = num_labels\n",
    "        config.id2label = id2label\n",
    "        config.label2id = label2id\n",
    "        \n",
    "        # Load the model with updated configuration\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "        \n",
    "       # model.classifier = torch.nn.Linear(model.bert.config.hidden_size, config.num_labels)\n",
    "       # model.classifier.apply(model._init_weights)\n",
    "        return model\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./models/opti',\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=datasetDict['train'],  # Ensure these datasets are correctly defined\n",
    "        eval_dataset=datasetDict['test'],\n",
    "        data_collator= data_collator,\n",
    "        compute_metrics=compute_metrics  # Define this function to compute your desired metrics\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train and evaluate the model\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        f1_score = eval_results['eval_f1']\n",
    "    finally:\n",
    "    # Ensure cleanup happens even if there are errors during training/evaluation\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, gc_after_trial = True)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d1b33-c56f-4663-9eb8-8e0f605b4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials = study.trials\n",
    "\n",
    "# Sort trials based on their achieved f1_scores\n",
    "sorted_trials = sorted(all_trials, key=lambda t: t.value, reverse=True)  # Assuming higher f1_score is better\n",
    "\n",
    "# Display sorted trials\n",
    "for trial in sorted_trials:\n",
    "    print(f\"Trial {trial.number} F1-score: {trial.value}, Parameters: {trial.params}\")\n",
    "#trainer.save_model(\"models/new_test_page/\")\n",
    "#print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17748a-f856-4631-b89c-2a839acadd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"KB/bert-base-swedish-cased-ner\"  # Example model, replace with your specific model\n",
    "        \n",
    "        \n",
    "# Update the config with the number of labels you have\n",
    "num_labels = len(label2id)  # This should be set up from your ClassLabel or similar configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_labels\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "# Load the model with updated configuration\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/nah',  # specify output directory              # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=8.118900895030543e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=9,\n",
    "    warmup_steps=361,\n",
    "    weight_decay=0.21913316520609985,\n",
    "    lr_scheduler_type=\"cosine\"\n",
    "    #**study.best_params\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, # make sure model is loaded and correctly configured for Token Classification\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer =tokenizer,\n",
    "    train_dataset=datasetDict['train'],\n",
    "    eval_dataset=datasetDict['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74057a3f-af0f-46f0-b3a1-6d945284592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models/finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76f40a-0a10-4399-96a7-68ed2c3ee5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/finished/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Test line\n",
    "test_line = \"\"\\n",
    "",
    "\n",
    "B al ans räkn ing Not 2016-12-31 2016-04-30\n",
    "EGET KAPITAL OCH SKULDER\n",
    "\n",
    "Eget kapital\n",
    "\n",
    "Bundet eget kapital\n",
    "\n",
    "Aktiekapital 1080000 1000000\n",
    "Reservfond 3300 3300\n",
    "Summa bundet eget kapital 1083300 1003300\n",
    "Fritt eget kapital\n",
    "\n",
    "Överkursfond 11920000 0\n",
    "Balanserat resultat 14725416 8713489\n",
    "Årets resultat -21226857 6011927\n",
    "Summa fritt eget kapital 5418559 14725416\n",
    "Summa eget kapital 6501859 15 '728716\n",
    "Långfristiga skulder |\n",
    "\n",
    "Övriga skulder till kreditinstitut 0 355564\n",
    "Skulder till koncernföretag 12900941 22243045\n",
    "Övriga skulder 79264641 0\n",
    "Summa långfristiga skulder 20865582 22598609\n",
    "Kortfristiga skulder\n",
    "\n",
    "Leverantörsskulder 484794 360235\n",
    "Skatteskulder 0 101316\n",
    "SMS-lån 10000 10000\n",
    "Övriga skulder 186722 71386\n",
    "Upplupna kostnader och förutbetalda intäkter 185566 65000\n",
    "Summa kortfristiga skulder 857082 597937\n",
    "SUMMA EGET KAPITAL OCH SKULDER 28224523 38925262\n",
    "\f",
    "\n",
    "\f",
    "\"\"\"\n",
    "tokenized_input = tokenizer(test_line, return_tensors=\"pt\")\n",
    "\n",
    "# Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_input)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    " # Adjust as per your setup\n",
    "\n",
    "# Convert indices to labels\n",
    "predicted_label_strings = [id2label[idx] for idx in predictions[0].tolist()]\n",
    "\n",
    "# Print tokens with their labels\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n",
    "print(\"Predicted NER tags:\")\n",
    "for token, label in zip(tokens, predicted_label_strings):\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d743c15-cba2-410c-885d-c56714efc134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Extract predictions and labels from the outputs\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "    true_labels = p.label_ids\n",
    "    \n",
    "    # Flatten the predictions and true_labels to handle them with sklearn metrics\n",
    "    # Also, remove ignored indices (-100 used for padding or special tokens)\n",
    "    true_labels_flat = [label for sublist in true_labels for label in sublist if label != -100]\n",
    "    predictions_flat = [pred for sublist, true_list in zip(predictions, p.label_ids)\n",
    "                        for pred, true in zip(sublist, true_list) if true != -100]\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_labels_flat, predictions_flat)\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cfff1-f31e-4762-b69d-2cd578abecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_file_path = 'modified-test-set-new.json'\n",
    "model_path = './models/finished/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "processed_data_2 = process_json_file(test_json_file_path, tokenizer, label2id)\n",
    "\n",
    "print(tokenizer.pad_token_id)\n",
    "test_dataset = Dataset.from_pandas(processed_data_2)\n",
    "\n",
    "\n",
    "dataset.set_format('torch')\n",
    " # now promt the model to do prediictions on the whole dataset\n",
    "# and calculate metrics maybe with the previously created compute_metrics()\n",
    "trainer = Trainer(model=model,\n",
    "                 compute_metrics=compute_metrics,\n",
    "                data_collator=data_collator)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "#predictions = trainer.predict(datasetDict['train'])\n",
    "#predictions = trainer.predict(datasetDict['test'])\n",
    "metrics = predictions.metrics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6979d4-ee81-433d-ac9f-0481b3c0601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef5330-cb51-4b5c-9e32-314d2a27a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['test_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad99ec1-f76a-4fc3-a27a-b67bfa1b2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('confusion_matrix.npy', metrics['test_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4117f81-00c2-46a6-9310-7ada0cbf5a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jesper_venv",
   "language": "python",
   "name": "jesper_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
