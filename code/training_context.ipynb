{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065abc1e-5853-4b09-8ded-d91d4694489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'label' key \n",
      "Missing 'label' key \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda2d728f57c400098726461df44aaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/442 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76389664bb804a45adc0de93ea28f65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/295 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994f56ddd7a245e0b8483e7095d91fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/442 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35ef7b4ade74c2fa5f1cc74365724b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/295 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/jesper_venv/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([14, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([14]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, AdamW\n",
    "from datasets import Dataset, ClassLabel\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Function to generate BIO tags with label validation\n",
    "def generate_bio_tags(text, labels, tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = tokenized_text[\"input_ids\"]\n",
    "    bio_tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    token_offsets = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_offsets_mapping=True\n",
    "    )[\"offset_mapping\"]\n",
    "\n",
    "    # Loop over entity labels to generate BIO tags\n",
    "    for entity in labels:\n",
    "        start = entity[\"start\"]\n",
    "        end = entity[\"end\"]\n",
    "        entity_label = entity[\"labels\"][0]\n",
    "\n",
    "        # Apply BIO tagging\n",
    "        start_idx = None\n",
    "        for i, offset in enumerate(token_offsets):\n",
    "            if offset[0] >= start and start_idx is None:\n",
    "                bio_tags[i] = \"B-\" + entity_label  # Apply \"B-\" prefix\n",
    "                start_idx = i\n",
    "            elif start_idx is not None and offset[0] < end:\n",
    "                bio_tags[i] = \"I-\" + entity_label  # Apply \"I-\" prefix\n",
    "            elif start_idx is not None and offset[0] >= end:\n",
    "                break\n",
    "\n",
    "    return tokens, bio_tags\n",
    "\n",
    "\n",
    "def chunk_tokens_and_labels(tokenized_text, bio_tags, chunk_size, tokenizer):\n",
    "    # Lists to store chunks of text and labels\n",
    "    text_chunks = []\n",
    "    label_chunks = []\n",
    "\n",
    "    padding_token = tokenizer.pad_token_id  # The token used for padding\n",
    "    padding_label = \"O\"  # Default label for padding\n",
    "\n",
    "    # Split the tokenized text and bio_tags into chunks\n",
    "    start_idx = 0\n",
    "\n",
    "    while start_idx < len(tokenized_text):\n",
    "        end_idx = min(start_idx + chunk_size, len(tokenized_text))\n",
    "\n",
    "        # Adjust end_idx to avoid splitting entities\n",
    "        if end_idx < len(tokenized_text):\n",
    "            while end_idx > start_idx and bio_tags[end_idx].startswith(\"I-\"):\n",
    "                end_idx -= 1\n",
    "\n",
    "        # Get the chunked text and labels\n",
    "        chunk_text = tokenized_text[start_idx:end_idx]\n",
    "        chunk_labels = bio_tags[start_idx:end_idx]\n",
    "\n",
    "        # Apply padding if the chunk is shorter than 128 tokens\n",
    "        if len(chunk_text) < chunk_size:\n",
    "            padding_length = chunk_size - len(chunk_text)\n",
    "            chunk_text += [padding_token] * padding_length\n",
    "            chunk_labels += [padding_label] * padding_length\n",
    "\n",
    "        text_chunks.append(chunk_text)\n",
    "        label_chunks.append(chunk_labels)\n",
    "\n",
    "        # Move the start index for the next chunk\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return text_chunks, label_chunks\n",
    "\n",
    "\n",
    "def prepare_sentences(json_data, tokenizer):\n",
    "    sentences = []\n",
    "\n",
    "    for item in json_data:\n",
    "        file_path = item[\"text\"]  # The file path to the text file\n",
    "        if \"label\" in item:\n",
    "            labels = item[\"label\"]\n",
    "        else:\n",
    "            print(f\"Missing 'label' key \")\n",
    "            continue\n",
    "\n",
    "        with open(f'balance_context/{file_path}', 'r') as txt_file:\n",
    "            text = txt_file.read()\n",
    "\n",
    "        # Ensure labels are correctly formatted\n",
    "        tokens, bio_tags = generate_bio_tags(text, labels, tokenizer)\n",
    "        text_rows, label_rows = chunk_tokens_and_labels(tokens, bio_tags, 128, tokenizer)\n",
    "\n",
    "        for text_row, label_row in zip(text_rows, label_rows):\n",
    "            sentence = {'text': tokenizer.decode(text_row), 'labels': label_row}\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def encode_labels(labels, num_classes, device):\n",
    "    encoded_labels = torch.zeros(labels.size(0), labels.size(1), num_classes, device=device)\n",
    "    for i in range(labels.size(0)):\n",
    "        for j in range(labels.size(1)):\n",
    "            encoded_labels[i, j, int(labels[i, j])] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Convert predicted labels to multi-class format if they are one-hot encoded\n",
    "    if len(preds.shape) > 1:\n",
    "        preds = preds.argmax(axis=1)\n",
    "\n",
    "    # Ensure labels are in a multi-class format\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = labels.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "classmap = ClassLabel(\n",
    "    num_classes=11,\n",
    "    names=[\n",
    "        'B-ASSET', 'I-ASSET', 'B-DEBT', 'I-DEBT', 'B-EQUITY', 'I-EQUITY',\n",
    "        'B-DEPOSIT', 'I-DEPOSIT', 'B-MISC', 'I-MISC', 'O'\n",
    "    ]\n",
    ")\n",
    "id2label = {i: classmap.int2str(i) for i in range(classmap.num_classes)}\n",
    "label2id = {classmap.str2int(c): c for c in classmap.names}\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    torch.cuda.empty_cache()\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        \"KB/bert-base-swedish-cased-ner\",\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        finetuning_task=\"ner\",\n",
    "        ignore_mismatched_sizes=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")\n",
    "\n",
    "\n",
    "# Modify the CustomTrainer class to use one-hot encoded labels\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/ner_model_output1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=300,\n",
    "    eval_steps=300,\n",
    "    learning_rate=2e-5,  # Consider experimenting with this value\n",
    "    num_train_epochs=5,  # Adjust based on performance\n",
    "    per_device_train_batch_size=32,  # Test smaller or larger sizes\n",
    "    warmup_steps=100,  # Adjust based on total number of steps\n",
    "    weight_decay=0.01,  # Increase if overfitting is observed\n",
    "    logging_steps=50,  # More frequent logs can help monitor training\n",
    "    load_best_model_at_end=True  # Load the best model at the end based on validation loss\n",
    "    # lr_scheduler_type='linear'  # Apply a learning rate scheduler\n",
    ")\n",
    "# Define the class mapping with accurate labels\n",
    "\n",
    "# Create id2label and label2id mappings based on classmap\n",
    "\n",
    "\n",
    "# Load the model with correct classifier initialization\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "\n",
    "# Load the JSON data\n",
    "with open('context_balance.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Prepare sentences\n",
    "sentences = prepare_sentences(json_data, tokenizer)\n",
    "\n",
    "# Create a dataset from the collected sentences\n",
    "ds = Dataset.from_pandas(pd.DataFrame(data=sentences))\n",
    "ds.set_format(\"torch\")\n",
    "datasetDict = ds.train_test_split(test_size=0.4)\n",
    "\n",
    "# Tokenize the text with proper truncation and padding\n",
    "ds_train = datasetDict.map(lambda x: tokenizer(x[\"text\"], truncation=True, max_length=128, padding='max_length'))\n",
    "# Use the classmap to convert label names to class indices\n",
    "ds_train = ds_train.map(lambda y: {\"labels\": classmap.str2int(y[\"labels\"])})\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "import optuna\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32, 64, 128]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 1000),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 10),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"polynomial\"]),\n",
    "        # Add more parameters as needed\n",
    "    }\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train['train'],\n",
    "    eval_dataset=ds_train['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#best_trial = trainer.hyperparameter_search(\n",
    "#    direction=\"maximize\",\n",
    "#    backend=\"optuna\",\n",
    "#    hp_space=optuna_hp_space,\n",
    "#    n_trials=20\n",
    "#)\n",
    "\n",
    "# Train the model\n",
    "#trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe5dd97-3cce-41af-a91b-da476bed9b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_trial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbest_trial\u001b[49m\u001b[38;5;241m.\u001b[39mhyperparameters)\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_trial' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_trial.hyperparameters)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dba6582-09dd-45dd-ae76-8acfc84f45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([14, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([14]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([14, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([14]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.481517</td>\n",
       "      <td>0.938983</td>\n",
       "      <td>0.909435</td>\n",
       "      <td>0.942706</td>\n",
       "      <td>0.938983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.070700</td>\n",
       "      <td>0.175707</td>\n",
       "      <td>0.938983</td>\n",
       "      <td>0.909435</td>\n",
       "      <td>0.942706</td>\n",
       "      <td>0.938983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.070700</td>\n",
       "      <td>0.069709</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.927453</td>\n",
       "      <td>0.958914</td>\n",
       "      <td>0.915254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84, training_loss=0.6977024248668126, metrics={'train_runtime': 46.1007, 'train_samples_per_second': 28.763, 'train_steps_per_second': 1.822, 'total_flos': 86626922982912.0, 'train_loss': 0.6977024248668126, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/ner_model_output\",\n",
    "    learning_rate=5.799352882984339e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay= 0.16,\n",
    "    warmup_steps= 100,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "      # Increase if overfitting is observed\n",
    "    logging_steps=50,  # More frequent logs can help monitor training\n",
    "    load_best_model_at_end=True,\n",
    "    #**best_trial.hyperparameters\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train['train'],\n",
    "    eval_dataset=ds_train['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ba98d8-3ce3-460c-aa27-0721cd217843",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21477973-2b21-423c-938d-7930b0287e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c151c28-b13a-44d2-a1c9-77f81c9c3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-EQUITY', 'score': 0.9469383, 'index': 53, 'word': 'kapital', 'start': 144, 'end': 151}, {'entity': 'I-EQUITY', 'score': 0.93538254, 'index': 54, 'word': 'Aktie', 'start': 152, 'end': 157}, {'entity': 'B-EQUITY', 'score': 0.85348624, 'index': 66, 'word': '##000', 'start': 192, 'end': 195}, {'entity': 'I-EQUITY', 'score': 0.72364104, 'index': 67, 'word': 'Reserv', 'start': 196, 'end': 202}, {'entity': 'B-EQUITY', 'score': 0.9704223, 'index': 85, 'word': 'kapital', 'start': 270, 'end': 277}, {'entity': 'I-EQUITY', 'score': 0.9526658, 'index': 86, 'word': 'Bal', 'start': 278, 'end': 281}, {'entity': 'I-EQUITY', 'score': 0.97080964, 'index': 87, 'word': '##anser', 'start': 281, 'end': 286}, {'entity': 'I-EQUITY', 'score': 0.9634478, 'index': 88, 'word': '##at', 'start': 286, 'end': 288}, {'entity': 'B-EQUITY', 'score': 0.89252377, 'index': 95, 'word': '##8', 'start': 310, 'end': 311}, {'entity': 'I-EQUITY', 'score': 0.9647261, 'index': 96, 'word': 'Årets', 'start': 312, 'end': 317}, {'entity': 'B-DEBT', 'score': 0.85852224, 'index': 123, 'word': '##80', 'start': 410, 'end': 412}, {'entity': 'I-DEBT', 'score': 0.50786656, 'index': 124, 'word': 'Peri', 'start': 413, 'end': 417}, {'entity': 'I-DEBT', 'score': 0.48332822, 'index': 125, 'word': '##odi', 'start': 417, 'end': 420}, {'entity': 'I-MISC', 'score': 0.5935558, 'index': 126, 'word': '##ser', 'start': 420, 'end': 423}, {'entity': 'I-MISC', 'score': 0.5458514, 'index': 127, 'word': '##ings', 'start': 423, 'end': 427}, {'entity': 'B-DEBT', 'score': 0.8812278, 'index': 137, 'word': 'skulder', 'start': 459, 'end': 466}, {'entity': 'I-DEBT', 'score': 0.9568955, 'index': 138, 'word': 'Lever', 'start': 467, 'end': 472}, {'entity': 'I-DEBT', 'score': 0.95097774, 'index': 139, 'word': '##ant', 'start': 472, 'end': 475}, {'entity': 'I-DEBT', 'score': 0.95182693, 'index': 140, 'word': '##örs', 'start': 475, 'end': 478}, {'entity': 'B-DEBT', 'score': 0.8913198, 'index': 146, 'word': '##56', 'start': 495, 'end': 497}, {'entity': 'I-DEBT', 'score': 0.9502715, 'index': 147, 'word': 'Skatte', 'start': 498, 'end': 504}, {'entity': 'B-DEBT', 'score': 0.9277868, 'index': 151, 'word': '##93', 'start': 517, 'end': 519}, {'entity': 'I-DEBT', 'score': 0.8904176, 'index': 152, 'word': 'Övriga', 'start': 520, 'end': 526}, {'entity': 'I-DEBT', 'score': 0.6851972, 'index': 153, 'word': 'kort', 'start': 527, 'end': 531}, {'entity': 'I-DEBT', 'score': 0.7480703, 'index': 154, 'word': '##frist', 'start': 531, 'end': 536}, {'entity': 'I-DEBT', 'score': 0.81056213, 'index': 155, 'word': '##iga', 'start': 536, 'end': 539}, {'entity': 'I-DEBT', 'score': 0.49727225, 'index': 160, 'word': '##55', 'start': 558, 'end': 560}, {'entity': 'B-DEBT', 'score': 0.87431735, 'index': 161, 'word': '##3', 'start': 560, 'end': 561}, {'entity': 'I-DEBT', 'score': 0.93582875, 'index': 162, 'word': 'Uppl', 'start': 562, 'end': 566}, {'entity': 'I-DEBT', 'score': 0.9395649, 'index': 163, 'word': '##up', 'start': 566, 'end': 568}, {'entity': 'I-DEBT', 'score': 0.9415008, 'index': 164, 'word': '##na', 'start': 568, 'end': 570}, {'entity': 'I-DEBT', 'score': 0.92933756, 'index': 165, 'word': 'kostnader', 'start': 571, 'end': 580}, {'entity': 'I-DEBT', 'score': 0.9022285, 'index': 166, 'word': 'och', 'start': 581, 'end': 584}, {'entity': 'I-DEBT', 'score': 0.9127494, 'index': 167, 'word': 'förut', 'start': 585, 'end': 590}, {'entity': 'I-DEBT', 'score': 0.9220438, 'index': 168, 'word': '##betalda', 'start': 590, 'end': 597}]\n",
      "Vintergatan Film TV AB 2018-06-30\n",
      "Org nr 556267-2658\n",
      "BALANSRÄKNING\n",
      "Not 2018-06-302017-06-30\n",
      "EGET KAPITAL OCH SKULDER\n",
      "Eget kapital 2\n",
      "Bundet eget \u001b[94mkapital\u001b[0m\n",
      "\u001b[94mAktie\u001b[0mkapital (1000 st å 50kr) 105000 105\u001b[94m000\u001b[0m\n",
      "\u001b[94mReserv\u001b[0mfond 21000 21000\n",
      "Summa bundet eget kapital 126000 126000\n",
      "Fritt eget \u001b[94mkapital\u001b[0m\n",
      "\u001b[94mBal\u001b[0m\u001b[94manser\u001b[0m\u001b[94mat\u001b[0m resultat 322680 14580\u001b[94m8\u001b[0m\n",
      "\u001b[94mÅrets\u001b[0m resultat -71881 176872\n",
      "Summa fritt eget kapital 250799 322680\n",
      "summa eget kapital 376799 4486\u001b[93m80\u001b[0m\n",
      "\u001b[93mPeri\u001b[0m\u001b[93modi\u001b[0m\u001b[93mser\u001b[0m\u001b[93mings\u001b[0mfond 73000 73000 |\n",
      "Kortfristiga \u001b[93mskulder\u001b[0m\n",
      "\u001b[93mLever\u001b[0m\u001b[93mant\u001b[0m\u001b[93mörs\u001b[0mskulder 12507 225\u001b[93m56\u001b[0m\n",
      "\u001b[93mSkatte\u001b[0mskulder 0 220\u001b[93m93\u001b[0m\n",
      "\u001b[93mÖvriga\u001b[0m \u001b[93mkort\u001b[0m\u001b[93mfrist\u001b[0m\u001b[93miga\u001b[0m skulder 113193 102\u001b[93m55\u001b[0m\u001b[93m3\u001b[0m\n",
      "\u001b[93mUppl\u001b[0m\u001b[93mup\u001b[0m\u001b[93mna\u001b[0m \u001b[93mkostnader\u001b[0m \u001b[93moch\u001b[0m \u001b[93mförut\u001b[0m\u001b[93mbetalda\u001b[0m intäkter 35000 134398\n",
      "Summa kortfristiga skulder 160700 281600\n",
      "SUMMA EGET KAPITAL OCH SKULDER 610499 803280\n",
      "STÄLLDA SÄKERHETER OCH ANSVARSFÖRBINDELSER\n",
      "ställda säkerheter Inga\n",
      "Ansvarsförbindelser Inga\n",
      "Na |\n",
      "\f",
      "\n",
      "\f",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = \"models/random\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "NER = pipeline('ner', model= model, tokenizer = tokenizer)\n",
    "\n",
    "# Function to make predictions on new text\n",
    "def predict(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the predicted labels\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=2)\n",
    "    # Decode the labels\n",
    "    decoded_labels = [model.config.id2label[label.item()] for label in predicted_labels[0]]\n",
    "    # Return the tokenized text and predicted labels\n",
    "    return tokenizer.decode(inputs.input_ids[0]), decoded_labels\n",
    "\n",
    "# Example text for prediction\n",
    "text_to_predict = \"\"\"Vintergatan Film TV AB 2018-06-30\n",
    "Org nr 556267-2658\n",
    "BALANSRÄKNING\n",
    "Not 2018-06-302017-06-30\n",
    "EGET KAPITAL OCH SKULDER\n",
    "Eget kapital 2\n",
    "Bundet eget kapital\n",
    "Aktiekapital (1000 st å 50kr) 105000 105000\n",
    "Reservfond 21000 21000\n",
    "Summa bundet eget kapital 126000 126000\n",
    "Fritt eget kapital\n",
    "Balanserat resultat 322680 145808\n",
    "Årets resultat -71881 176872\n",
    "Summa fritt eget kapital 250799 322680\n",
    "summa eget kapital 376799 448680\n",
    "Periodiseringsfond 73000 73000 |\n",
    "Kortfristiga skulder\n",
    "Leverantörsskulder 12507 22556\n",
    "Skatteskulder 0 22093\n",
    "Övriga kortfristiga skulder 113193 102553\n",
    "Upplupna kostnader och förutbetalda intäkter 35000 134398\n",
    "Summa kortfristiga skulder 160700 281600\n",
    "SUMMA EGET KAPITAL OCH SKULDER 610499 803280\n",
    "STÄLLDA SÄKERHETER OCH ANSVARSFÖRBINDELSER\n",
    "ställda säkerheter Inga\n",
    "Ansvarsförbindelser Inga\n",
    "Na |\n",
    "\f",
    "\n",
    "\f",
    "\n",
    "\f",
    "\"\"\"\n",
    "\n",
    "# Make prediction\n",
    "#tokenized_text, predicted_labels = predict(text_to_predict)\n",
    "#print(\"Tokenized text:\", tokenized_text)\n",
    "#print(\"Predicted labels:\", predicted_labels)\n",
    "l = []\n",
    "for token in NER(text_to_predict):\n",
    "    if token['word'].startswith('##'):\n",
    "        l[-1]['word'] += token['word'][2:]\n",
    "    else:\n",
    "        l += [ token ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_ner_results(text, predictions):\n",
    "    last_end = 0\n",
    "    result_text = \"\"\n",
    "    # Sort predictions by the start position\n",
    "    predictions.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        start, end = prediction['start'], prediction['end']\n",
    "        entity = prediction['entity']\n",
    "        word = text[start:end]\n",
    "        \n",
    "        # Append text from last entity to current entity start\n",
    "        result_text += text[last_end:start]\n",
    "        \n",
    "        # Apply color coding to entity\n",
    "        if 'ASSET' in entity:\n",
    "            # Green for ASSETS\n",
    "            result_text += f\"\\033[92m{word}\\033[0m\"\n",
    "        elif 'EQUITY' in entity:\n",
    "            # Blue for EQUITY\n",
    "            result_text += f\"\\033[94m{word}\\033[0m\"\n",
    "        else:\n",
    "            # Yellow for others\n",
    "            result_text += f\"\\033[93m{word}\\033[0m\"\n",
    "        \n",
    "        last_end = end\n",
    "    \n",
    "    # Append any remaining text after the last entity\n",
    "    result_text += text[last_end:]\n",
    "    \n",
    "    print(result_text)\n",
    "\n",
    "# Example usage\n",
    "print(NER(text_to_predict))\n",
    "print_ner_results(text_to_predict, NER(text_to_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4749b-f975-408d-92f5-705275e130dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d0816-a9cf-41b5-be28-1ca95e66ce3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jesper_venv",
   "language": "python",
   "name": "jesper_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
