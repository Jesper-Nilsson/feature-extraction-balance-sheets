{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065abc1e-5853-4b09-8ded-d91d4694489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, AdamW\n",
    "from datasets import Dataset, ClassLabel\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Function to generate BIO tags with label validation\n",
    "def generate_bio_tags(text, labels, tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = tokenized_text[\"input_ids\"]\n",
    "    bio_tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    token_offsets = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_offsets_mapping=True\n",
    "    )[\"offset_mapping\"]\n",
    "\n",
    "    # Loop over entity labels to generate BIO tags\n",
    "    for entity in labels:\n",
    "        start = entity[\"start\"]\n",
    "        end = entity[\"end\"]\n",
    "        entity_label = entity[\"labels\"][0]\n",
    "\n",
    "        # Apply BIO tagging\n",
    "        start_idx = None\n",
    "        for i, offset in enumerate(token_offsets):\n",
    "            if offset[0] >= start and start_idx is None:\n",
    "                bio_tags[i] = \"B-\" + entity_label  # Apply \"B-\" prefix\n",
    "                start_idx = i\n",
    "            elif start_idx is not None and offset[0] < end:\n",
    "                bio_tags[i] = \"I-\" + entity_label  # Apply \"I-\" prefix\n",
    "            elif start_idx is not None and offset[0] >= end:\n",
    "                break\n",
    "\n",
    "    return tokens, bio_tags\n",
    "\n",
    "\n",
    "def chunk_tokens_and_labels(tokenized_text, bio_tags, chunk_size, tokenizer):\n",
    "    # Lists to store chunks of text and labels\n",
    "    text_chunks = []\n",
    "    label_chunks = []\n",
    "\n",
    "    padding_token = tokenizer.pad_token_id  # The token used for padding\n",
    "    padding_label = \"O\"  # Default label for padding\n",
    "\n",
    "    # Split the tokenized text and bio_tags into chunks\n",
    "    start_idx = 0\n",
    "\n",
    "    while start_idx < len(tokenized_text):\n",
    "        end_idx = min(start_idx + chunk_size, len(tokenized_text))\n",
    "\n",
    "        # Adjust end_idx to avoid splitting entities\n",
    "        if end_idx < len(tokenized_text):\n",
    "            while end_idx > start_idx and bio_tags[end_idx].startswith(\"I-\"):\n",
    "                end_idx -= 1\n",
    "\n",
    "        # Get the chunked text and labels\n",
    "        chunk_text = tokenized_text[start_idx:end_idx]\n",
    "        chunk_labels = bio_tags[start_idx:end_idx]\n",
    "\n",
    "        # Apply padding if the chunk is shorter than 128 tokens\n",
    "        if len(chunk_text) < chunk_size:\n",
    "            padding_length = chunk_size - len(chunk_text)\n",
    "            chunk_text += [padding_token] * padding_length\n",
    "            chunk_labels += [padding_label] * padding_length\n",
    "\n",
    "        text_chunks.append(chunk_text)\n",
    "        label_chunks.append(chunk_labels)\n",
    "\n",
    "        # Move the start index for the next chunk\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return text_chunks, label_chunks\n",
    "\n",
    "\n",
    "def prepare_sentences(json_data, tokenizer):\n",
    "    sentences = []\n",
    "\n",
    "    for item in json_data:\n",
    "        file_path = item[\"text\"]  # The file path to the text file\n",
    "        if \"label\" in item:\n",
    "            labels = item[\"label\"]\n",
    "        else:\n",
    "            print(f\"Missing 'label' key \")\n",
    "            continue\n",
    "\n",
    "        with open(f'balance_context/{file_path}', 'r') as txt_file:\n",
    "            text = txt_file.read()\n",
    "\n",
    "        # Ensure labels are correctly formatted\n",
    "        tokens, bio_tags = generate_bio_tags(text, labels, tokenizer)\n",
    "        text_rows, label_rows = chunk_tokens_and_labels(tokens, bio_tags, 128, tokenizer)\n",
    "\n",
    "        for text_row, label_row in zip(text_rows, label_rows):\n",
    "            sentence = {'text': tokenizer.decode(text_row), 'labels': label_row}\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def encode_labels(labels, num_classes, device):\n",
    "    encoded_labels = torch.zeros(labels.size(0), labels.size(1), num_classes, device=device)\n",
    "    for i in range(labels.size(0)):\n",
    "        for j in range(labels.size(1)):\n",
    "            encoded_labels[i, j, int(labels[i, j])] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Convert predicted labels to multi-class format if they are one-hot encoded\n",
    "    if len(preds.shape) > 1:\n",
    "        preds = preds.argmax(axis=1)\n",
    "\n",
    "    # Ensure labels are in a multi-class format\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = labels.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "classmap = ClassLabel(\n",
    "    num_classes=11,\n",
    "    names=[\n",
    "        'B-ASSET', 'I-ASSET', 'B-DEBT', 'I-DEBT', 'B-EQUITY', 'I-EQUITY',\n",
    "        'B-DEPOSIT', 'I-DEPOSIT', 'B-MISC', 'I-MISC', 'O'\n",
    "    ]\n",
    ")\n",
    "id2label = {i: classmap.int2str(i) for i in range(classmap.num_classes)}\n",
    "label2id = {classmap.str2int(c): c for c in classmap.names}\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    torch.cuda.empty_cache()\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        \"KB/bert-base-swedish-cased-ner\",\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        finetuning_task=\"ner\",\n",
    "        ignore_mismatched_sizes=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased-ner\")\n",
    "\n",
    "\n",
    "# Modify the CustomTrainer class to use one-hot encoded labels\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/ner_model_output1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=300,\n",
    "    eval_steps=300,\n",
    "    learning_rate=2e-5,  # Consider experimenting with this value\n",
    "    num_train_epochs=5,  # Adjust based on performance\n",
    "    per_device_train_batch_size=32,  # Test smaller or larger sizes\n",
    "    warmup_steps=100,  # Adjust based on total number of steps\n",
    "    weight_decay=0.01,  # Increase if overfitting is observed\n",
    "    logging_steps=50,  # More frequent logs can help monitor training\n",
    "    load_best_model_at_end=True  # Load the best model at the end based on validation loss\n",
    "    # lr_scheduler_type='linear'  # Apply a learning rate scheduler\n",
    ")\n",
    "# Define the class mapping with accurate labels\n",
    "\n",
    "# Create id2label and label2id mappings based on classmap\n",
    "\n",
    "\n",
    "# Load the model with correct classifier initialization\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "\n",
    "# Load the JSON data\n",
    "with open('context_balance.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Prepare sentences\n",
    "sentences = prepare_sentences(json_data, tokenizer)\n",
    "\n",
    "# Create a dataset from the collected sentences\n",
    "ds = Dataset.from_pandas(pd.DataFrame(data=sentences))\n",
    "ds.set_format(\"torch\")\n",
    "datasetDict = ds.train_test_split(test_size=0.4)\n",
    "\n",
    "# Tokenize the text with proper truncation and padding\n",
    "ds_train = datasetDict.map(lambda x: tokenizer(x[\"text\"], truncation=True, max_length=128, padding='max_length'))\n",
    "# Use the classmap to convert label names to class indices\n",
    "ds_train = ds_train.map(lambda y: {\"labels\": classmap.str2int(y[\"labels\"])})\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "import optuna\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32, 64, 128]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 1000),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 10),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"polynomial\"]),\n",
    "        # Add more parameters as needed\n",
    "    }\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train['train'],\n",
    "    eval_dataset=ds_train['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#best_trial = trainer.hyperparameter_search(\n",
    "#    direction=\"maximize\",\n",
    "#    backend=\"optuna\",\n",
    "#    hp_space=optuna_hp_space,\n",
    "#    n_trials=20\n",
    "#)\n",
    "\n",
    "# Train the model\n",
    "#trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5dd97-3cce-41af-a91b-da476bed9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_trial.hyperparameters)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba6582-09dd-45dd-ae76-8acfc84f45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/ner_model_output\",\n",
    "    learning_rate=5.799352882984339e-05,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay= 0.16,\n",
    "    warmup_steps= 100,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "      # Increase if overfitting is observed\n",
    "    logging_steps=50,  # More frequent logs can help monitor training\n",
    "    load_best_model_at_end=True,\n",
    "    #**best_trial.hyperparameters\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train['train'],\n",
    "    eval_dataset=ds_train['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba98d8-3ce3-460c-aa27-0721cd217843",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4749b-f975-408d-92f5-705275e130dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d0816-a9cf-41b5-be28-1ca95e66ce3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jesper_venv",
   "language": "python",
   "name": "jesper_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
